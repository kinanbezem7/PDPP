# PDPP
Pinterest Data Processing Pipeline

MILESTONE 1
Set up the conda environment for the pipeline 

MILESTONE 2
Research the requirements for the project. 
Identify key technologies to use. 

MILESTONE 3
Run the Api and Pinterest user emulation code. 

MILESTONE 4
Create Kafka topic, Send events from API to Kafka topic, 
Create Kafka consumers for batch and streaming. 

MILESTONE 5
Send data to s3 bucket from the batch consumer. 

MILESTONE 6
Process data from s3 bucket using Spark. 

MILESTONE 7
Send data to Cassandra for storage

MILESTONE 8
Set-up Presto run adhoc queries from Cassandra data. 

MILESTONE 9 
Set-up Apache Airflow to time the batch consumer to run every 24 hours. 

MILESTONE 10 
Set-up Prometheus to monitor Cassandra 

MILESTONE 11
Consume data using Spark-streaming 

MILESTONE 12
Transform the streaming data with Spark

MILESTONE 13
Store the data from the streaming consumer in PGAdmin 4 

MILESTONE 14
Monitor the postgres database using prometheus
